---
layout: post
comments: true
title:  "TLoL: Human level in League of Legends using Deep Learning (Grok)"
excerpt: "Some excerpt"
date:   2025-11-28 00:00:00
categories: [Project]
tags: ["League of Legends", "Machine Learning", "Reinforcement Learning", "Grok", "LLMs"]
---

## Table of Contents

* TOC
{:toc}

## Introduction

Recently there has been a massive development for League of Legends game playing AI...

<div style="text-align: center;">
    <blockquote class="twitter-tweet"><p lang="en" dir="ltr">Let’s see if <a href="https://twitter.com/grok?ref_src=twsrc%5Etfw">@Grok</a> 5 can beat the best human team <a href="https://twitter.com/LeagueOfLegends?ref_src=twsrc%5Etfw">@LeagueOfLegends</a> in 2026 with these important constraints:<br><br>1. Can only look at the monitor with a camera, seeing no more than what a person with 20/20 vision would see. <br><br>2. Reaction latency and click rate no faster than human.…</p>&mdash; Elon Musk (@elonmusk) <a href="https://twitter.com/elonmusk/status/1993208505486979327?ref_src=twsrc%5Etfw">November 25, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>

Elon Musk has issued a public challenge to beat the worlds best League of Legends team by the end of 2026 using Grok 5.
There are also two very interesting restrictions for the challenge:
1. Can only look at the monitor with a camera, seeing no more than what a person with 20/20 vision would see.
2. Reaction latency and click rate no faster than human.

The second criteria is to avoid the issue of the bot simply out "APMing" humans, in the sense that the bot has higher mechanical precision and reaction
speeds and can therefore mathematically calculate the perfect way of avoiding damage. Most would agree this isn't a display of interesting behaviour,
but rather a "hydraulic press" way of beating humans.

However the first criteria is more interesting, that the input into the bot is simply just the raw visual feed from the game. This departs from
other frontier game playing systems such as OpenAI Five and AlphaStar which were provided raw data from the game using an API. This means that
Grok 5 will also have to learn how to process visual information, segment different objects in view and many other complex visual processing skills.
Presumably Grok will also have the same viewport restrictions which humans have, so the bot would also have to pan the camera over to areas of interest
as well??

<div style="text-align: center;">
   <img
      src="/assets/lol/openai_five_arch.png"
      style="width: 100%; max-width: 1024px;"
   />
</div>

Above we can see the architecture of OpenAI Five, the architecture of the system which beat the world champions at Dota 2 in 2019. The
section in blue is dedicated purely to processing each of the individual units in the game and directly integrates complex spatial-visual
information, along with semantic information per each unit. Grok 5 will have to learn to process similar visual information for League of Legends.

## Feasibility?

The key question for this project is, within the next year is it feasible that a visual large language model (vLLM) such as Grok 5 could genuinely
beat the best League of Legends team in the world? Let's assess this question by asking what beating the worlds best team actually means.
Grok 5 will presumably have to control five agents within League of Legends, one for each role (Top, Jungle, Mid, ADC and Support) with the aforementioned
restrictions. Aside from that, the system has no other restrictions.

So, has this type of thing been done before? Yes, multiple MOBA-style games have had AI-based systems beat professional teams, ranging from Dota 2 ([OpenAI Five](https://openai.com/index/openai-five/)),
Honor of Kings - which is a game very similar to League of Legends ([JueWu-SL](https://miscellaneousstuff.github.io/project/2021/09/04/tlol-part-4-exploring-the-literature.html)), etc.

However, the nature of Grok 5 being the agent along with the purely visual input changes things drastically. Some contributors on X (formerly Twitter) have
already built out environments using Claude 4.5 Opus (currently SOTA for LLM-based agentic control systems) to play League of Legends.

<div style="text-align: center;">
    <blockquote class="twitter-tweet" data-media-max-width="560"><p lang="en" dir="ltr">Can computer-use models play games now, one-shot?<br><br>I gave Claude Opus 4.5 a simple prompt like &quot;play league of legends&quot; and it starts clicking and typing around my computer pretty effectively even though it doesn&#39;t win due to latency<br><br>More interestingly between Minecraft, finding… <a href="https://t.co/jSXf6E6a95">pic.twitter.com/jSXf6E6a95</a></p>&mdash; Surya Dantuluri (@sdand) <a href="https://twitter.com/sdand/status/1993820274387767533?ref_src=twsrc%5Etfw">November 26, 2025</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>

From this we can already see some interesting strengths and weaknesses from this SOTA (state-of-the-art) system:
- Mechanical precision: there is a clear lack of mechanical precision in the movement orders issued by Claude
- Object segmentation: the system is struggling to precisely identify different objects in the view
- Speed: the main limitation is the lack of a real-time control loop due to the time-to-first-token and the time taken by claude to generate the text based responses which encode the reasoning and the final action prediction

However there are some impressive trends:
- Learning: the agent is learning from its experience in real time. It's adjusting its policy in real time based on feedback
- Generalisation: despite having not been trained to play league of legends, it is somewhat successfully controlling the player and issuing commands

This vein of work follows a growing body of work aimed at using LLMs to play games as a test of their utilisation of world-knowledge, on-going learning abilities and
general curiosity, such as:
- [Gemini plays Pokemon](https://www.twitch.tv/gemini_plays_pokemon) - Gemini 3 Pro playing Pokemon Crystal
- [Claude plays Pokemon](https://www.twitch.tv/claudeplayspokemon) - Claude Opus 4.5 playing Pokemon Red
- [Minecraft Voyager](https://github.com/MineDojo/Voyager) - LLM-based agent learns to play minecraft by adapting python policy based on game feedback

The common theme among these projects is:
- Vision: the pokemon related projects use purely vision inputs into Gemini and Claude as input
- Motor control: The systems expose a simpler action paradigm for the agents (aside from Voyager) which is essentially tool use. However for Minecraft Voyager, it also has a simpler
set of basic actions, however the agent learns to flexibly compose these into hierarchies of complex tool use to be able to accomplish the in-game achievements such as acquiring diamonds

So between the existing LLM agentic control projects (Pokemon, Voyager, etc.), the existing Opus 4.5 League of Legends agent and history of successful MOBA game playing AI systems, 
it stands to reason that producing a bot which can play League of Legends is entirely feasible. However this brings up the next question, can Grok 5 learn to beat the world champions
at League of Legends, and what would be required to achieve this?

## Existing Work

The first place to look would be existing work into League of Legends game playing AI. Earlier in this blog, we focused strongly on a project called TLoL. This was my attempt
at producing human level League of Legends AI. Ultimately that project concluded prematurely due to the sheer difficulty of the task and the nature of its execution.

League of Legends patch 4.20:
- [pylol](https://github.com/MiscellaneousStuff/pylol) - League of Legends patch 4.20 reinforcment learning environment
- [pylol-demo](https://colab.research.google.com/github/MiscellaneousStuff/pylol-demo/blob/68e9f8dbdd54b822d710c879fcfe1e249b55c620/demonstration.ipynb) - First open-source League of Legends RL environment, successfully trains a basic agent using [PPO](https://arxiv.org/pdf/1707.06347) in Google Colab!
- [lolgym](https://github.com/jjlee0802cu/lolgym) - Adversarial multi-agent system trained to outperform hardcoded bot and humans in a 1v1 environment (from Columbia University students: Mustafa Eyceoz and Justin Lee)

League of Legends season 11 to 13:
- [TLoL RL](https://github.com/MiscellaneousStuff/tlol-rl) - League of Legends Season 13 reinforcement learning environment
- [tlol-py](https://github.com/MiscellaneousStuff/tlol-py) - Replay scraping orchestration, conversion of extracted datasets into datasets for analysis and model training, etc.
- [TLoL](https://github.com/MiscellaneousStuff/tlol) - League of Legends season 11 to 13 game playing AI datasets (10k+ games)
- [tlol-llm](https://github.com/MiscellaneousStuff/tlol-llm) - Very early attempts at analysing League of Legends games using LLMs to test understanding